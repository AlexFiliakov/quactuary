{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended Distribution Support Examples\n",
    "\n",
    "This notebook demonstrates the extended distribution support in quActuary, including:\n",
    "- Compound Binomial distributions\n",
    "- Mixed Poisson processes\n",
    "- Zero-inflated models\n",
    "- Edgeworth expansions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Import quActuary distributions\n",
    "from quactuary.distributions.frequency import Poisson, Binomial, NegativeBinomial\n",
    "from quactuary.distributions.severity import Exponential, Gamma, LogNormal\n",
    "from quactuary.distributions.compound_binomial import (\n",
    "    BinomialExponentialCompound, BinomialGammaCompound, BinomialLognormalCompound\n",
    ")\n",
    "from quactuary.distributions.mixed_poisson import (\n",
    "    PoissonGammaMixture, PoissonInverseGaussianMixture, HierarchicalPoissonMixture\n",
    ")\n",
    "from quactuary.distributions.zero_inflated import ZeroInflatedCompound, detect_zero_inflation\n",
    "from quactuary.distributions.edgeworth import EdgeworthExpansion\n",
    "from quactuary.distributions.compound_extensions import create_extended_compound_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Compound Binomial Distributions\n",
    "\n",
    "Useful when the number of claims has an upper bound (e.g., fixed number of policies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Insurance portfolio with 100 policies\n",
    "# Each policy has 30% chance of claim, average claim size $5,000\n",
    "\n",
    "n_policies = 100\n",
    "claim_prob = 0.3\n",
    "avg_claim = 5000\n",
    "\n",
    "# Create compound distribution\n",
    "freq = Binomial(n=n_policies, p=claim_prob)\n",
    "sev = Exponential(scale=avg_claim)\n",
    "compound = BinomialExponentialCompound(freq, sev)\n",
    "\n",
    "print(f\"Expected total claims: ${compound.mean():,.2f}\")\n",
    "print(f\"Standard deviation: ${compound.std():,.2f}\")\n",
    "print(f\"95% VaR: ${compound.ppf(0.95):,.2f}\")\n",
    "print(f\"99% VaR: ${compound.ppf(0.99):,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different severity distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Exponential severity\n",
    "compound_exp = BinomialExponentialCompound(freq, Exponential(scale=avg_claim))\n",
    "x = np.linspace(0, 300000, 1000)\n",
    "axes[0].plot(x, compound_exp.pdf(x), 'b-', lw=2)\n",
    "axes[0].set_title('Binomial-Exponential')\n",
    "axes[0].set_xlabel('Total Loss')\n",
    "axes[0].set_ylabel('Density')\n",
    "\n",
    "# Gamma severity\n",
    "compound_gamma = BinomialGammaCompound(freq, Gamma(a=2, scale=avg_claim/2))\n",
    "axes[1].plot(x, compound_gamma.pdf(x), 'g-', lw=2)\n",
    "axes[1].set_title('Binomial-Gamma')\n",
    "axes[1].set_xlabel('Total Loss')\n",
    "\n",
    "# Lognormal severity\n",
    "compound_lognorm = BinomialLognormalCompound(freq, LogNormal(s=1, scale=avg_claim))\n",
    "axes[2].plot(x, compound_lognorm.pdf(x), 'r-', lw=2)\n",
    "axes[2].set_title('Binomial-Lognormal')\n",
    "axes[2].set_xlabel('Total Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mixed Poisson Processes\n",
    "\n",
    "Capture heterogeneity in risk exposure across the portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Heterogeneous portfolio with varying risk levels\n",
    "# Risk parameter λ ~ Gamma(α=3, β=0.5)\n",
    "\n",
    "mixed_poisson = PoissonGammaMixture(alpha=3, beta=0.5)\n",
    "\n",
    "# Compare with standard Poisson\n",
    "standard_poisson = Poisson(mu=mixed_poisson.mean())\n",
    "\n",
    "print(\"Comparison of Poisson vs Mixed Poisson:\")\n",
    "print(f\"Mean (both): {mixed_poisson.mean():.2f}\")\n",
    "print(f\"Variance - Poisson: {standard_poisson._dist.var():.2f}\")\n",
    "print(f\"Variance - Mixed: {mixed_poisson.var():.2f}\")\n",
    "print(f\"Overdispersion factor: {mixed_poisson.var() / mixed_poisson.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PMF comparison\n",
    "k_values = np.arange(0, 25)\n",
    "pmf_standard = standard_poisson.pmf(k_values)\n",
    "pmf_mixed = mixed_poisson.pmf(k_values)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(k_values - 0.2, pmf_standard, width=0.4, label='Standard Poisson', alpha=0.7)\n",
    "plt.bar(k_values + 0.2, pmf_mixed, width=0.4, label='Mixed Poisson (NB)', alpha=0.7)\n",
    "plt.xlabel('Number of Claims')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Standard vs Mixed Poisson: Capturing Heterogeneity')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical model for portfolio with multiple business lines\n",
    "hierarchical = HierarchicalPoissonMixture(\n",
    "    portfolio_alpha=2.0,\n",
    "    portfolio_beta=0.3,\n",
    "    group_alpha=4.0,\n",
    "    n_groups=5  # 5 business lines\n",
    ")\n",
    "\n",
    "# Simulate portfolio structure\n",
    "sim_results = hierarchical.simulate_portfolio(size=1000, random_state=42)\n",
    "\n",
    "print(\"Portfolio Simulation Results:\")\n",
    "print(f\"Average total claims: {np.mean(sim_results['total']):.2f}\")\n",
    "print(f\"Std dev of total claims: {np.std(sim_results['total']):.2f}\")\n",
    "print(\"\\nAverage claims by business line:\")\n",
    "for i in range(5):\n",
    "    print(f\"  Line {i+1}: {np.mean(sim_results['by_group'][:, i]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Zero-Inflated Models\n",
    "\n",
    "Handle datasets with excess zeros (e.g., many policies with no claims)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data with zero-inflation\n",
    "np.random.seed(42)\n",
    "n_obs = 1000\n",
    "true_zero_prob = 0.3  # 30% structural zeros\n",
    "\n",
    "# Base distributions\n",
    "freq_base = Poisson(mu=3.0)\n",
    "sev_base = Gamma(a=2, scale=1000)\n",
    "\n",
    "# Generate zero-inflated data\n",
    "is_structural_zero = np.random.rand(n_obs) < true_zero_prob\n",
    "data = np.zeros(n_obs)\n",
    "\n",
    "for i in range(n_obs):\n",
    "    if not is_structural_zero[i]:\n",
    "        n_claims = freq_base.rvs()\n",
    "        if n_claims > 0:\n",
    "            data[i] = np.sum(sev_base.rvs(size=n_claims))\n",
    "\n",
    "print(f\"Proportion of zeros in data: {np.mean(data == 0):.3f}\")\n",
    "print(f\"Expected without zero-inflation: {np.exp(-3):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect zero-inflation\n",
    "is_zi, diagnostics = detect_zero_inflation(data, freq_base, sev_base)\n",
    "\n",
    "print(\"Zero-Inflation Detection Results:\")\n",
    "print(f\"Zero-inflated: {is_zi}\")\n",
    "print(f\"Observed zero proportion: {diagnostics['observed_zero_proportion']:.3f}\")\n",
    "print(f\"Expected zero proportion: {diagnostics['expected_zero_proportion']:.3f}\")\n",
    "print(f\"Excess zeros: {diagnostics['excess_zeros']:.3f}\")\n",
    "print(f\"Score statistic: {diagnostics['score_statistic']:.2f}\")\n",
    "print(f\"P-value: {diagnostics['p_value']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit zero-inflated model\n",
    "zi_model = ZeroInflatedCompound(freq_base, sev_base)\n",
    "fit_result = zi_model.fit_em(data, max_iter=50)\n",
    "\n",
    "print(\"\\nEM Algorithm Results:\")\n",
    "print(f\"Estimated zero-inflation probability: {fit_result['zero_prob']:.3f}\")\n",
    "print(f\"True zero-inflation probability: {true_zero_prob:.3f}\")\n",
    "print(f\"Converged: {fit_result['converged']}\")\n",
    "print(f\"Iterations: {fit_result['iterations']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Edgeworth Expansion\n",
    "\n",
    "Approximate distributions using moment-based corrections to the normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Moderate skewness and kurtosis\n",
    "mean = 10000\n",
    "variance = 2500000  # std = 1581\n",
    "skewness = 0.8\n",
    "excess_kurtosis = 0.5\n",
    "\n",
    "# Create Edgeworth expansion\n",
    "edgeworth = EdgeworthExpansion(\n",
    "    mean=mean,\n",
    "    variance=variance,\n",
    "    skewness=skewness,\n",
    "    excess_kurtosis=excess_kurtosis\n",
    ")\n",
    "\n",
    "# Validate expansion\n",
    "validation = edgeworth.validate_expansion(order=4)\n",
    "print(\"Edgeworth Expansion Validation:\")\n",
    "for key, value in validation.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Edgeworth approximation with normal\n",
    "x = np.linspace(mean - 4*np.sqrt(variance), mean + 4*np.sqrt(variance), 1000)\n",
    "\n",
    "# Normal approximation\n",
    "normal_pdf = stats.norm.pdf(x, loc=mean, scale=np.sqrt(variance))\n",
    "\n",
    "# Edgeworth approximations of different orders\n",
    "edge_pdf_2 = edgeworth.pdf(x, order=2)  # Just normal\n",
    "edge_pdf_3 = edgeworth.pdf(x, order=3)  # With skewness\n",
    "edge_pdf_4 = edgeworth.pdf(x, order=4)  # With skewness and kurtosis\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x, normal_pdf, 'b--', label='Normal', lw=2)\n",
    "plt.plot(x, edge_pdf_3, 'g-', label='Edgeworth (order 3)', lw=2)\n",
    "plt.plot(x, edge_pdf_4, 'r-', label='Edgeworth (order 4)', lw=2)\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Edgeworth Expansion vs Normal Approximation')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantile comparison\n",
    "quantiles = [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]\n",
    "\n",
    "normal_quantiles = stats.norm.ppf(quantiles, loc=mean, scale=np.sqrt(variance))\n",
    "edge_quantiles_cf = edgeworth.ppf(quantiles, order=4, method='cornish-fisher')\n",
    "edge_quantiles_num = edgeworth.ppf(quantiles, order=4, method='numerical')\n",
    "\n",
    "print(\"Quantile Comparison:\")\n",
    "print(\"q\\tNormal\\tEdge(CF)\\tEdge(Num)\")\n",
    "for i, q in enumerate(quantiles):\n",
    "    print(f\"{q:.2f}\\t{normal_quantiles[i]:.0f}\\t{edge_quantiles_cf[i]:.0f}\\t\\t{edge_quantiles_num[i]:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Integrated Example: Full Analysis Pipeline\n",
    "\n",
    "Combine all features for a comprehensive actuarial analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario: Auto insurance portfolio analysis\n",
    "# - Mixed customer risk levels (mixed Poisson)\n",
    "# - Some customers never claim (zero-inflation)\n",
    "# - Need accurate tail estimates (Edgeworth)\n",
    "\n",
    "# Step 1: Create extended compound distribution\n",
    "compound_model = create_extended_compound_distribution(\n",
    "    frequency='poisson',\n",
    "    severity='gamma',\n",
    "    zero_inflated=True,\n",
    "    zero_prob=0.15,\n",
    "    use_edgeworth=False,  # Will compare later\n",
    "    mu=2.5,  # Average 2.5 claims per year\n",
    "    a=2.0,   # Gamma shape\n",
    "    scale=3000  # Average claim $6000\n",
    ")\n",
    "\n",
    "# Step 2: Generate sample data\n",
    "np.random.seed(42)\n",
    "portfolio_size = 5000\n",
    "simulated_losses = compound_model.rvs(size=portfolio_size)\n",
    "\n",
    "# Step 3: Analyze results\n",
    "print(\"Portfolio Analysis Results:\")\n",
    "print(f\"Number of policies: {portfolio_size}\")\n",
    "print(f\"Policies with no claims: {np.sum(simulated_losses == 0)} ({100*np.mean(simulated_losses == 0):.1f}%)\")\n",
    "print(f\"Average loss per policy: ${np.mean(simulated_losses):,.2f}\")\n",
    "print(f\"Total portfolio loss: ${np.sum(simulated_losses):,.2f}\")\n",
    "print(\"\\nRisk Metrics:\")\n",
    "print(f\"95% VaR: ${np.percentile(simulated_losses, 95):,.2f}\")\n",
    "print(f\"99% VaR: ${np.percentile(simulated_losses, 99):,.2f}\")\n",
    "print(f\"99.5% VaR: ${np.percentile(simulated_losses, 99.5):,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Compare approximation methods\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Histogram of simulated data\n",
    "ax1.hist(simulated_losses[simulated_losses > 0], bins=50, density=True, \n",
    "         alpha=0.7, label='Simulated Data')\n",
    "ax1.set_xlabel('Loss Amount')\n",
    "ax1.set_ylabel('Density')\n",
    "ax1.set_title('Distribution of Non-Zero Losses')\n",
    "ax1.set_xlim(0, 50000)\n",
    "\n",
    "# Q-Q plot for tail behavior\n",
    "from scipy.stats import probplot\n",
    "probplot(simulated_losses[simulated_losses > 0], dist=\"norm\", plot=ax2)\n",
    "ax2.set_title('Q-Q Plot: Checking Normality in Tails')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Step 5: Premium calculation\n",
    "safety_loading = 0.2  # 20% safety margin\n",
    "pure_premium = np.mean(simulated_losses)\n",
    "loaded_premium = pure_premium * (1 + safety_loading)\n",
    "\n",
    "print(f\"\\nPremium Calculation:\")\n",
    "print(f\"Pure premium: ${pure_premium:,.2f}\")\n",
    "print(f\"Safety loading: {safety_loading*100:.0f}%\")\n",
    "print(f\"Loaded premium: ${loaded_premium:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Compound Binomial Distributions**: For bounded claim counts\n",
    "2. **Mixed Poisson Processes**: For heterogeneous portfolios\n",
    "3. **Zero-Inflated Models**: For excess zeros in data\n",
    "4. **Edgeworth Expansion**: For accurate tail approximations\n",
    "5. **Integrated Analysis**: Combining features for real-world applications\n",
    "\n",
    "These tools enable more accurate modeling of complex actuarial scenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}